{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac8edc4",
   "metadata": {},
   "source": [
    "# Transfer Learning NLP. Ulises Bértolo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e409f",
   "metadata": {},
   "source": [
    "### Creando el Dataset y la instancia del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7aae8e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (C:\\Users\\200248\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e93ce4dd2243a48c6599940ced0fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Loading our dataset\n",
    "tweet_dataset = load_dataset(path=\"tweet_eval\", name=\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "238b36fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a24a070eca46c0b4705aea35ccf02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2733ee6b140c984f5f5a6a0b4394c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our DistilBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e08254",
   "metadata": {},
   "source": [
    "### Preproceso de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e2f1f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3257\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1421\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 374\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5616b14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3257\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72798931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence samples:\n",
      " [\"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\", \"My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs\"]\n",
      "\n",
      "Label samples:\n",
      " [2, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence samples:\\n {tweet_dataset['train']['text'][:2]}\\n\")\n",
    "print(f\"Label samples:\\n {tweet_dataset['train']['label'][:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40499511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0: \"anger\", 1: \"joy\", 2: \"optimism\", 3: \"sadness\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df6067aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for finding the length of the longest sequence in the data\n",
    "def find_max_length(dataset):\n",
    "    return len(max(dataset, key=lambda x: len(x.split())).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71bba4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence in train set has 33 words\n",
      "Longest sequence in val set has 32 words\n",
      "Longest sequence in test set has 36 words\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the length of the longest sequences in our data splits\n",
    "train_max_length = find_max_length(tweet_dataset[\"train\"][\"text\"])\n",
    "val_max_length = find_max_length(tweet_dataset[\"validation\"][\"text\"])\n",
    "test_max_length = find_max_length(tweet_dataset[\"test\"][\"text\"])\n",
    "\n",
    "# Inspecting the length of the longest sequences\n",
    "print(f\"Longest sequence in train set has {train_max_length} words\")\n",
    "print(f\"Longest sequence in val set has {val_max_length} words\")\n",
    "print(f\"Longest sequence in test set has {test_max_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6dc3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for discarding sequences beyond a specified length\n",
    "def filter_dataset(dataset, num_words):    \n",
    "    return dataset.filter(lambda x: len(x[\"text\"].split()) <= num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1c12633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c020a8152e504ce48ba22593a73bf04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2642384fdd644e12a1ab666fb0d260e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d376ac03e004b23a84ff27de1d29fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specifying the max length for sequences\n",
    "num_words = 36\n",
    "\n",
    "# Dropping sequences longer than the specified number from the dataset\n",
    "filtered_dataset = filter_dataset(tweet_dataset, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "198ffd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the shortened dataset\n",
    "print(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21478ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for tokenizing our dataset\n",
    "def tokenize_dataset(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", \n",
    "                     truncation=True, max_length=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef7f81c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83ed64ba2374aa6a90a6d5c309061e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18be6350588476790d41714177ba7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3553b5b758094bc89bb4abeac12f999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing our dataset\n",
    "tokenized_dataset = filtered_dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da70cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the tokenized dataset\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3989e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\", 'label': 2, 'input_ids': [101, 1523, 4737, 2003, 1037, 2091, 7909, 2006, 1037, 3291, 2017, 2089, 2196, 2031, 1005, 1012, 11830, 11527, 1012, 1001, 14354, 1001, 4105, 1001, 4737, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a training sample\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f96b10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"text\" and \"label\" columns from our data splits to craft features for the model\n",
    "train_features = tokenized_dataset[\"train\"].remove_columns([\"text\", \"label\"]).with_format(\"tensorflow\")\n",
    "val_features = tokenized_dataset[\"validation\"].remove_columns([\"text\", \"label\"]).with_format(\"tensorflow\")\n",
    "test_features = tokenized_dataset[\"test\"].remove_columns([\"text\", \"label\"]).with_format(\"tensorflow\")\n",
    "\n",
    "# Converting our features to TF Tensors\n",
    "train_features = {x: train_features[x] for x in tokenizer.model_input_names}\n",
    "val_features = {x: val_features[x] for x in tokenizer.model_input_names}\n",
    "test_features = {x: test_features[x] for x in tokenizer.model_input_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b6757c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Inspecting expected model input names\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c63db566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3257, 36), dtype=int64, numpy=\n",
      "array([[  101,  1523,  4737, ...,     0,     0,     0],\n",
      "       [  101,  2026, 18328, ...,     0,     0,     0],\n",
      "       [  101,  2053,  2021, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  1030,  5310, ...,     0,     0,     0],\n",
      "       [  101,  2017,  2031, ...,     0,     0,     0],\n",
      "       [  101,  1030,  5310, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(3257, 36), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n"
     ]
    }
   ],
   "source": [
    "# Inspecting our Tensors\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33885446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Importing the function for one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Creating labels for each of the data splits\n",
    "train_labels = to_categorical(tokenized_dataset[\"train\"][\"label\"])\n",
    "val_labels = to_categorical(tokenized_dataset[\"validation\"][\"label\"])\n",
    "test_labels = to_categorical(tokenized_dataset[\"test\"][\"label\"])\n",
    "\n",
    "\n",
    "# Inspecting training labels\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60f544ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the TF Dataset class\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Creating TF Datasets for each of our data splits\n",
    "train_dataset = Dataset.from_tensor_slices((train_features, train_labels))\n",
    "val_dataset = Dataset.from_tensor_slices((val_features, val_labels))\n",
    "test_dataset = Dataset.from_tensor_slices((test_features, test_labels))\n",
    "\n",
    "# Shuffling and batching our data\n",
    "train_dataset = train_dataset.shuffle(len(train_features), seed=2).batch(8)\n",
    "val_dataset = val_dataset.shuffle(len(train_features), seed=2).batch(8)\n",
    "test_dataset = test_dataset.shuffle(len(train_features), seed=2).batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a14a7",
   "metadata": {},
   "source": [
    "### Applying fine tuning to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbfb36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,956,548\n",
      "Trainable params: 66,956,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5c9e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the DistilBERT block\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f448d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,956,548\n",
      "Trainable params: 593,668\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the model again to see the differences in trainable params\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be82f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function defining our learning rate schedule\n",
    "def lr_decay(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1 * epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81294791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating our learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule=lr_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e77df24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters and compiling the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics=tf.keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3aaaed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/15\n",
      "408/408 [==============================] - 166s 385ms/step - loss: 0.9809 - categorical_accuracy: 0.5981 - val_loss: 0.8611 - val_categorical_accuracy: 0.6310 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/15\n",
      "408/408 [==============================] - 159s 389ms/step - loss: 0.8232 - categorical_accuracy: 0.6681 - val_loss: 0.8645 - val_categorical_accuracy: 0.6364 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/15\n",
      "408/408 [==============================] - 168s 412ms/step - loss: 0.8102 - categorical_accuracy: 0.6838 - val_loss: 0.8241 - val_categorical_accuracy: 0.6337 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/15\n",
      "408/408 [==============================] - 167s 409ms/step - loss: 0.7818 - categorical_accuracy: 0.6936 - val_loss: 0.8449 - val_categorical_accuracy: 0.6364 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/15\n",
      "408/408 [==============================] - 159s 390ms/step - loss: 0.7493 - categorical_accuracy: 0.7028 - val_loss: 0.8217 - val_categorical_accuracy: 0.6471 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/15\n",
      "408/408 [==============================] - 152s 374ms/step - loss: 0.7446 - categorical_accuracy: 0.7108 - val_loss: 0.8051 - val_categorical_accuracy: 0.6631 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/15\n",
      "408/408 [==============================] - 149s 365ms/step - loss: 0.7307 - categorical_accuracy: 0.7135 - val_loss: 0.8235 - val_categorical_accuracy: 0.6497 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/15\n",
      "408/408 [==============================] - 147s 361ms/step - loss: 0.7207 - categorical_accuracy: 0.7169 - val_loss: 0.8330 - val_categorical_accuracy: 0.6310 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/15\n",
      "408/408 [==============================] - 169s 414ms/step - loss: 0.7069 - categorical_accuracy: 0.7228 - val_loss: 0.8322 - val_categorical_accuracy: 0.6524 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/15\n",
      "408/408 [==============================] - 182s 446ms/step - loss: 0.6860 - categorical_accuracy: 0.7237 - val_loss: 0.8588 - val_categorical_accuracy: 0.6364 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0003678794586447782.\n",
      "Epoch 11/15\n",
      "408/408 [==============================] - 152s 373ms/step - loss: 0.6525 - categorical_accuracy: 0.7446 - val_loss: 0.7858 - val_categorical_accuracy: 0.6765 - lr: 3.6788e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00012245643455377955.\n",
      "Epoch 12/15\n",
      "408/408 [==============================] - 151s 370ms/step - loss: 0.6174 - categorical_accuracy: 0.7627 - val_loss: 0.7880 - val_categorical_accuracy: 0.6818 - lr: 1.2246e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 3.688316883663751e-05.\n",
      "Epoch 13/15\n",
      "408/408 [==============================] - 166s 406ms/step - loss: 0.6078 - categorical_accuracy: 0.7657 - val_loss: 0.7877 - val_categorical_accuracy: 0.6765 - lr: 3.6883e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 1.0051835886692629e-05.\n",
      "Epoch 14/15\n",
      "408/408 [==============================] - 177s 433ms/step - loss: 0.5924 - categorical_accuracy: 0.7694 - val_loss: 0.7878 - val_categorical_accuracy: 0.6765 - lr: 1.0052e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 2.4787521134827945e-06.\n",
      "Epoch 15/15\n",
      "408/408 [==============================] - 156s 383ms/step - loss: 0.6023 - categorical_accuracy: 0.7670 - val_loss: 0.7876 - val_categorical_accuracy: 0.6738 - lr: 2.4788e-06\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, \n",
    "                    epochs=15, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0143ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 62s 349ms/step - loss: 0.6970 - categorical_accuracy: 0.7333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.696970522403717, 0.733286440372467]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating our model on the test set\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
